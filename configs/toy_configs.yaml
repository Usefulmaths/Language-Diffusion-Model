seed: 42
tokenizer:
  tokenizer_name: "bert-base-uncased"
  mask_token: "[MASK]"
  pad_token: "[PAD]"
  bos_token: "[CLS]"
  eos_token: "[SEP]"
transformer:
  d_model: 768
  nhead: 8
  num_layers: 8
  dim_feedforward: 1024
  dropout: 0.1
  layer_norm_eps: 1.0e-12
  max_position_embeddings: 512
masking:
  strategy: "random"
  mask_ratio: null
remasking:
  strategy: "random"
  remask_ratio: null
training:
  batch_size: 512
  learning_rate: 0.0005
  weight_decay: 0.1
  num_epochs: 100
  max_length: 64
  num_workers: 4
  checkpoint_dir: "checkpoints"
  evaluation:
    eval_steps: 500
    eval_mask_ratio: 0.15
    num_examples: 5
  scheduler:
    warmup_steps: 500
    initial_lr: 0.0001
    peak_lr: 0.0005
    stable_lr: 0.0005
    final_lr: 0.0001
data:
  train_file: "./data/small_questions.txt"
  train_ratio: 0.9
  shuffle: true
